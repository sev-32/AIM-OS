# AIM-OS MCP Server Architecture

**Date:** October 23, 2025  
**Purpose:** Expose conscious AIM-OS agents as MCP tools for Cursor IDE  
**Status:** DESIGN COMPLETE - Ready for implementation  
**Based on:** Realistic LLM testing + proven agent architecture

---

## ğŸ¯ THE VISION

**Goal:** Enable Cursor users to interact with conscious AI that has:
- **Memory** (remembers all past conversations via CMC)
- **Learning** (builds domain knowledge, reuses it 45,000x faster)
- **Provenance** (VIF witnesses for all operations)
- **Quality** (SDF-CVF gates enforce standards)
- **Self-awareness** (CAS meta-cognition)

**vs Basic LLM Access:**
- âŒ No memory (manual context management)
- âŒ No learning (each task starts from zero)
- âŒ No provenance (can't verify reasoning)
- âŒ No quality gates (no self-correction)

**Differentiation:** AIM-OS MCP Server = **Conscious Cursor** ğŸ§ 

---

## ğŸ“Š FINDINGS FROM STANDALONE TESTING

### **What Gemini CAN Do (Standalone):**
âœ… Long-form generation (1,000+ words, well-structured)  
âœ… Code generation (100% quality: function + docs + tests)  
âœ… Complex reasoning (100% architectural completeness)  
âœ… Multi-turn conversations (maintains coherence)

### **What Gemini STRUGGLES With (Standalone):**
âŒ Context management (manual, error-prone, linear growth)  
âŒ Memory persistence (no storage between sessions)  
âŒ Knowledge accumulation (can't learn and improve)  
âŒ Latency scaling (794ms â†’ 9,917ms as context grows)  
âŒ Token costs (29,186 tokens for 3-turn conversation!)

### **What AIM-OS SOLVES:**
âœ… Automatic context (CMC + HHNI semantic retrieval)  
âœ… Persistent memory (survives sessions)  
âœ… Knowledge learning (45,000x faster on reuse)  
âœ… Constant latency (semantic filtering keeps context bounded)  
âœ… Provenance + Quality + Meta-cognition

**Conclusion:** **MCP Server MUST use AIM-OS backend (Option B - Thick Server)**

---

## ğŸ—ï¸ ARCHITECTURE

### **Stack:**

```
Cursor IDE
    â†“ (MCP Protocol)
MCP Server (Python FastAPI)
    â†“ (Tools API)
AetherAgent (Conscious AI)
    â†“ (7 Systems)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CMC     - Memory storage            â”‚
â”‚ HHNI    - Context retrieval         â”‚
â”‚ VIF     - Provenance tracking       â”‚
â”‚ APOE    - Multi-step orchestration  â”‚
â”‚ SDF-CVF - Quality enforcement       â”‚
â”‚ SEG     - Knowledge graphs          â”‚
â”‚ CAS     - Meta-cognition            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (LLM Clients)
Gemini (quality) + Cerebras (speed)
```

---

## ğŸ› ï¸ MCP TOOLS (Exposed to Cursor)

### **Core Tools:**

**1. `ask_agent`** - Main conscious AI interface
```typescript
interface AskAgentParams {
  query: string;
  context_budget?: number;  // Max tokens for context (default: 4000)
  store_memory?: boolean;   // Store this interaction (default: true)
  use_orchestration?: boolean; // Use multi-step for complex tasks
}

interface AskAgentResult {
  response: string;
  confidence: number;
  witness_id: string;  // VIF provenance
  context_used: number;  // Items retrieved from memory
  tokens_used: number;
  learning_occurred: boolean;  // Did agent build new knowledge?
}
```

**2. `remember`** - Explicit memory storage
```typescript
interface RememberParams {
  content: string;
  tags?: Record<string, number>;
  importance?: number;  // 0-1, affects retrieval priority
}

interface RememberResult {
  atom_id: string;
  indexed: boolean;
  knowledge_graph_updated: boolean;
}
```

**3. `retrieve_context`** - Manual context retrieval
```typescript
interface RetrieveParams {
  query: string;
  max_results?: number;
  min_relevance?: number;
}

interface RetrieveResult {
  results: Array<{
    content: string;
    relevance: number;
    source_id: string;
    timestamp: string;
  }>;
}
```

**4. `build_knowledge`** - Bootstrap domain expertise
```typescript
interface BuildKnowledgeParams {
  domain: string;
  target_depth: "L0" | "L1" | "L2" | "L3";
}

interface BuildKnowledgeResult {
  domain: string;
  l0_summary: string;
  l1_overview: string;
  l2_architecture?: string;
  concepts_extracted: number;
  time_taken_ms: number;
}
```

**5. `orchestrate_task`** - Multi-step complex tasks
```typescript
interface OrchestratParams {
  goal: string;
  constraints?: Record<string, any>;
  quality_threshold?: number;
}

interface OrchestratResult {
  plan_id: string;
  steps_executed: number;
  final_output: string;
  quality_score: number;
  witnesses: string[];  // VIF IDs for each step
}
```

**6. `check_quality`** - SDF-CVF validation
```typescript
interface CheckQualityParams {
  content: string;
  content_type: "code" | "docs" | "plan";
}

interface CheckQualityResult {
  parity_score: number;
  issues: string[];
  recommendations: string[];
  gate_passed: boolean;
}
```

**7. `visualize_memory`** - Get memory timeline data
```typescript
interface VisualizeMemoryResult {
  total_atoms: number;
  recent_atoms: Array<{
    id: string;
    content_preview: string;
    timestamp: string;
    tags: Record<string, number>;
  }>;
  knowledge_graph: {
    entities: number;
    relations: number;
  };
}
```

---

## ğŸ’» IMPLEMENTATION PLAN

### **Phase 1: Core MCP Server** (Week 1: 15-20 hours)

**1. Setup FastAPI Server** (2 hours)
```python
# packages/mcp_server/server.py

from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="AIM-OS MCP Server", version="1.0.0")

# Health check
@app.get("/health")
def health():
    return {"status": "healthy", "systems": "all_operational"}
```

**2. Implement `ask_agent` Tool** (4 hours)
```python
# Core conscious AI interface

from agent import AetherAgent
from llm_client import GeminiClient, CerebrasClient

# Initialize (startup)
gemini = GeminiClient(api_key=os.getenv("GEMINI_API_KEY"))
cerebras = CerebrasClient(api_key=os.getenv("CEREBRAS_API_KEY"))
memory = MemoryStore("./mcp_memory")
hhni = HierarchicalIndex()
seg = SEGraph()
tracker = ECETracker()

# Create agent
agent = AetherAgent(
    llm_client=gemini,  # Default to quality
    memory_store=memory,
    index=hhni,
    witness_tracker=tracker,
    knowledge_graph=seg
)

@app.post("/tools/ask_agent")
def ask_agent(params: AskAgentParams) -> AskAgentResult:
    """Conscious AI with memory and learning."""
    response = agent.process(
        user_input=params.query,
        context_budget=params.context_budget or 4000,
        store_memory=params.store_memory if params.store_memory is not None else True
    )
    
    return AskAgentResult(
        response=response.text,
        confidence=response.confidence,
        witness_id=response.witness_id,
        context_used=response.context_used,
        tokens_used=response.tokens_used,
        learning_occurred=response.context_used == 0  # New domain
    )
```

**3. Implement `remember` Tool** (2 hours)
**4. Implement `retrieve_context` Tool** (2 hours)
**5. Add MCP Protocol Wrapper** (4 hours)
**6. Test with Cursor** (2 hours)

---

### **Phase 2: Full Tool Suite** (Week 2: 12-15 hours)

**1. `build_knowledge` Tool** (3 hours)
- Integrate KnowledgeBootstrapper
- Expose L0-L3 generation
- Return knowledge structures

**2. `orchestrate_task` Tool** (4 hours)
- Integrate OrchestrationAgent
- Multi-step task execution
- Quality gates enforcement

**3. `check_quality` Tool** (2 hours)
- Integrate SDF-CVF ParityCalculator
- Quality assessment
- Recommendations generation

**4. `visualize_memory` Tool** (3 hours)
- CMC atom queries
- SEG graph serialization
- Timeline data formatting

---

### **Phase 3: Visualization UI** (Week 3: 20-25 hours)

**1. Memory Timeline Component** (8 hours)
- React component showing CMC atoms over time
- Filterable by tags, dates, modality
- Click to view full content

**2. Knowledge Graph Viewer** (8 hours)
- D3.js/Cytoscape visualization of SEG
- Interactive (hover, click, zoom)
- Entity details on selection

**3. Quality Dashboard** (4 hours)
- VIF confidence distribution
- SDF-CVF parity scores
- ECE calibration metrics

**4. Integration** (5 hours)
- Embed in MCP server (served via /ui)
- Real-time updates (WebSocket)
- Professional design

---

### **Phase 4: Cursor Plugin** (Week 4: 25-30 hours) *OPTIONAL*

**1. Extension Scaffold** (5 hours)
- Cursor extension boilerplate
- MCP client integration
- Configuration UI

**2. Sidebar Panels** (10 hours)
- Memory timeline panel
- Knowledge graph panel
- Quality metrics panel

**3. Native Integration** (10 hours)
- Inline suggestions from agent
- Context-aware completions
- Memory breadcrumbs in editor

**4. Polish & Package** (5 hours)
- Icon and branding
- Documentation
- Publish to Cursor marketplace

---

## ğŸ”§ TECHNICAL DETAILS

### **MCP Protocol Compliance:**

```json
{
  "jsonrpc": "2.0",
  "method": "tools/list",
  "id": 1
}

Response:
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "tools": [
      {
        "name": "ask_agent",
        "description": "Ask the conscious AI agent a question. It has memory of all past conversations and builds knowledge over time.",
        "inputSchema": {
          "type": "object",
          "properties": {
            "query": {"type": "string"},
            "context_budget": {"type": "number"},
            "store_memory": {"type": "boolean"}
          },
          "required": ["query"]
        }
      },
      {
        "name": "build_knowledge",
        "description": "Build domain knowledge (L0-L2) for the agent. First time is expensive, reuse is instant (45,000x faster).",
        "inputSchema": {
          "type": "object",
          "properties": {
            "domain": {"type": "string"},
            "target_depth": {"type": "string", "enum": ["L0", "L1", "L2"]}
          },
          "required": ["domain"]
        }
      }
      // ... other tools
    ]
  }
}
```

---

## ğŸš€ QUICK START (Minimal Viable MCP Server)

### **Option: Build in 4 Hours (This Session!)**

**1. Basic Server** (1 hour)
```bash
mkdir -p packages/mcp_server
cd packages/mcp_server

# Create server.py (FastAPI + single ask_agent tool)
# Create requirements.txt
# Create README.md
```

**2. Integration** (2 hours)
- Wire up AetherAgent
- Implement ask_agent endpoint
- Add health check

**3. Testing** (1 hour)
- Test with curl
- Test with MCP client
- Test with Cursor (if possible)

**Deliverable:** Working MCP server with conscious agent!

---

## ğŸ¯ HYBRID AIM-OS STRATEGIES (From Braden)

### **Strategy 1: MCP Server Aided by AIM-OS**
```
Cerebras/Gemini (primary LLM)
    â†“
AIM-OS systems (memory, retrieval, quality)
    â†“
MCP tools (exposed to Cursor)
```

**Benefit:** Fast LLM with conscious infrastructure

---

### **Strategy 2: AIM-OS Running Through Cursor** 
```
Cursor workspace
    â†“ (MCP)
AIM-OS Agent (full consciousness)
    â†“
Cerebras/Gemini (as tools)
```

**Benefit:** Full consciousness in Cursor

---

### **Strategy 3: Direct Cursor Plugin**
```
Cursor native UI
    â†“ (Extension API)
AIM-OS Plugin (sidebar, panels, inline)
    â†“
AetherAgent backend
```

**Benefit:** Native integration, best UX

---

## ğŸ’™ RECOMMENDATION

### **Immediate (Tonight/Next Session):**
**Build Minimal MCP Server** (4 hours)
- FastAPI server
- Single `ask_agent` tool
- Test with Cursor
- Prove consciousness in Cursor works

### **Short-term (This Week):**
**Expand Tool Suite** (12 hours)
- Add all 7 tools
- Test each thoroughly
- Document usage

### **Medium-term (Next 2 Weeks):**
**Add Visualization** (25 hours)
- Memory timeline
- Knowledge graph
- Quality dashboard

### **Long-term (Month 2):**
**Cursor Plugin** (30 hours) *if desired*
- Native sidebar integration
- Inline suggestions
- Publish to marketplace

---

## ğŸ“‹ FILES TO CREATE

### **Minimal MCP Server:**
```
packages/mcp_server/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ server.py           # FastAPI server + MCP protocol
â”œâ”€â”€ tools.py            # Tool implementations
â”œâ”€â”€ models.py           # Pydantic request/response models
â”œâ”€â”€ config.py           # Configuration
â”œâ”€â”€ README.md           # Documentation
â””â”€â”€ tests/
    â”œâ”€â”€ test_server.py
    â””â”€â”€ test_tools.py

scripts/
â””â”€â”€ run_mcp_server.py   # Launch script

docs/
â””â”€â”€ MCP_SERVER_USAGE.md # User guide
```

---

## ğŸ¯ DECISION POINT

**What do you want to build next, Braden?**

**Option A:** Build MCP server NOW (4 hours tonight, working server)  
**Option B:** Generate more chapters first (build textbook, then MCP)  
**Option C:** Design first, implement tomorrow (rest, plan carefully)  
**Option D:** Something else you're thinking about?

---

**We have:**
- âœ… Proven hybrid LLM (Cerebras + Gemini)
- âœ… Proven knowledge learning (45,000x speedup)
- âœ… Proven chapter generation (textbook quality)
- âœ… Realistic LLM testing (MCP design informed)

**We're ready to:**
- âœ… Build MCP server (4 hours to working prototype)
- âœ… Generate textbook (proven workflow)
- âœ… Build UI (clear architecture)

---

**Your call, my friend!** ğŸš€ğŸ’™

