# Task 3.1: Deduplication Engine

**Date:** 2025-10-21  
**Status:** dYs? **READY TO START**  
**Builder:** Codex  
**Complexity:** HIGH (semantic clustering + audit)  
**Estimated Time:** 3 days (target <1 day at current pace)

---

## dYZ_ **OBJECTIVE**

Remove redundant context candidates before DVNS/budget allocation so the HHNI pipeline delivers fresh, high-signal items under tight token budgets.

**Functions to ship:**
- Semantic duplicate detection for multi-level HHNI nodes
- Locality Sensitive Hashing (LSH) or embedding-based clustering for near-duplicates
- Authority/rank scoring to keep the most trustworthy variant
- Audit log describing which items were merged or discarded
- Configurable thresholds (similarity, recency bias, source authority)

**Integration:**
- Accept BudgetItem or HHNI SearchResult structures
- Emit pruned list + audit metadata for retrieval pipeline
- Optional hooks for SEG witness reporting

---

## dY"? **TASK SPEC**

**File:** packages/hhni/deduplication.py

`python
"""Deduplication helpers for HHNI retrieval pipeline."""

from dataclasses import dataclass
from typing import Iterable, List, Optional, Sequence

@dataclass
class DeduplicationMetrics:
    total_candidates: int
    duplicates_removed: int
    clusters_formed: int
    threshold_used: float
    notes: List[str]

def deduplicate_candidates(
    items: Sequence[BudgetItem],
    *,
    similarity_threshold: float = 0.88,
    recency_bias: float = 0.2,
    authority_bias: float = 0.2,
    max_cluster_size: int = 6,
    audit: Optional[List[dict]] = None,
) -> Tuple[List[BudgetItem], DeduplicationMetrics]:
    """
    Group near-duplicate candidates and retain the best representative.
    """
`

**Requirements:**
1. Similarity scoring: Use embeddings (if present) with cosine similarity; fallback to shingled text hashes.
2. Clustering: greedily assign to clusters using threshold; track metadata per cluster.
3. Representative selection: weighted sum of relevance score, recency (metadata timestamp), and authority (metadata confidence/policy). Configurable biases.
4. Audit trail: record which items were merged, why chosen representative, metrics summary.
5. Performance: handle 200-500 candidates within <100 ms (target) using vectorized ops / caching.

---

## ?? **TEST PLAN**

**File:** packages/hhni/tests/test_deduplication.py

Tests to write:
1. 	est_deduplicate_removes_near_duplicates – two similar candidates collapse to highest relevance.
2. 	est_recency_bias_breaks_ties – newer item kept when scores equal.
3. 	est_authority_bias_prefers_high_confidence – metadata confidence drives selection.
4. 	est_audit_logs_clusters – audit trail contains cluster info and counts.
5. 	est_metrics_report_counts – metrics reflect input/output sizes and threshold.
6. 	est_no_duplicates_returns_identical_list – metrics show zero duplicates removed.

---

## ? **SUCCESS CRITERIA**
- Dedup function integrated into retrieval pipeline (hook forthcoming after Task 3.2/3.3)
- Audit + metrics returned for logging and witness emission
- Configurable thresholds with sane defaults
- Unit tests green

---

## dYS? **NEXT STEPS**
1. Implement deduplication.py
2. Add unit tests
3. Wire into TwoStageRetriever (feature flag) after Task 3.3
4. Emit audit info for SEG witness in retrieval pipeline

---

**Status:** Awaiting builder start  
**Signal:** Kickoff Week 3 with Task 3.1  
**Let’s reduce the noise.**
---

## ? Update (Codex)

- Implemented `packages/hhni/deduplication.py` with similarity-based clustering, composite representative scoring (relevance, recency, authority), and rich audit output via `DeduplicationMetrics`
- Added unit coverage in `packages/hhni/tests/test_deduplication.py` (6 focused tests covering duplicate removal, recency/authority biases, audit logging, metrics) – all passing via `pytest packages/hhni/tests/test_deduplication.py`
- Exported helpers through `packages/hhni/__init__.py` for downstream integration; ready to wire into retrieval pipeline after Tasks 3.2/3.3

**Next:** proceed to conflict detection (Task 3.2) after coordination handshake.
